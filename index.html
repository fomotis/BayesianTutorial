<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>A Taste of Bayesian Inference</title>
    <meta charset="utf-8" />
    <meta name="author" content="Olusoji Oluwafemi Daniel, Owokotomo Olajumoke Evangelina" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/fc.css" rel="stylesheet" />
    <link href="libs/remark-css/robot-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">





background-image: url(pics/pic1.png)
background-position: 4% 0%
background-size: 350px


class: inverse, right, bottom

# A Taste of Bayesian Inference
## with examples from Ecology using RStan
&lt;br&gt;
### Olusoji Oluwafemi Daniel `\(^{1,2}\)`, Owokotomo Olajumoke Evangelina `\(^{1}\)` &lt;br&gt; `\(^1\)`Center for Statistics, Data Science Institute, Hasselt University, &lt;br&gt; `\(^2\)`Research Unit in Evolutionary Biology (URBE), Universite De Namur. &lt;br&gt;oluwafemi.olusoji@uhasselt.be, oluwafemi.olusoji@unamur.be&lt;br&gt; https://github.com/fomotis/BayesianTutorial

---

class: center

# Before we get started

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pics/frequentists_vs_bayesians.png" alt="Just for fun" width="50%" height="50%" /&gt;
&lt;p class="caption"&gt;Just for fun&lt;/p&gt;
&lt;/div&gt;

---

class: center

# Schools of thought in statistics

.pull-left[

`Frequentists`

  - main figures = Ronald Fisher, Jerzy Neyman &amp; Ergon Pearson
  &lt;br&gt;
  - main tools = p-value, confidence interval
  &lt;br&gt;
  - ideology based on repetition

]


.pull-right[
`Bayesians`
  - main figures = Thomas Bayes, Simon Laplace
    &lt;br&gt;
  - main tool = Baye's rule/theorem 
  &lt;br&gt;
  - ideology based on `\(postrior \propto likelihood \times prior\)`

]

* There is also the likelihoodist, but likelihoodist + frequentist = classical approach to statistics.

---

# Bayes theorem

`\(p(B|A) = \frac{p(A|B) p(B)}{ p(A|B) p(B) + p(A|B^c) p(B^c)}\)` or 
`\(p(\theta | y) = \frac{L(\theta | y) p(\theta)}{\int L(\theta | y) p(\theta)}\)`

&lt;img src="pics/equations.png" width="70%" height="70%" style="display: block; margin: auto;" /&gt;

The theory that would not die. How Bayes rule cracked the enigma code, hunted down Russian submarines &amp; emerged triumphant from two centuries of controversy, Mc Grayne (2011).

---

# Major ingredients for Bayesian inference

`Notations:` `\(\theta = parameter\)`, `\(y = data\)`, `\(p(.) = probability \  distribution\)`

## The prior

* can be based on historical data, personal belief or properties of the parameters of interest

* equivalent to extra data depicting your belief about `\(\theta\)`

* this concept forms the core of the criticsms against Byaesian methods 

`Practically:` you have to express the prior in terms of a probability distribution, i.e. `\(\theta \sim p(\theta)\)`.

---

# Major ingredients for Bayesian inference

## The likelihood

* `\(L(\theta| y)\)` is obtained from the data you have at hand.

`Practically:` `\(y\)` is assumed to follow some distribution, i.e. `\(y \sim p(y, \theta)\)`.

## The posterior

`\(p(\theta | y) \propto L(\theta| y) \times p(\theta)\)`

* it contains all information about `\(\theta\)`

* involves integration which can be quite complex even for the simplest of models.

`Practically:` we use sampling or approximation methods to obtain samples from the posterior distribution, `\(p(\theta | y)\)`

- sampling: Gibbs sampler (OpenBUGS, JAGS), MCMC sampling `(RStan)`

- laplace approximations: INLA (Integrated Nested Laplace Approximations)

---

# Posterior summary measures

Often, we use the follwoing summary measures as estimates for `\(\theta\)`

## posterior mean

`\(\bar{\theta} = \int_{\theta} \theta p(\theta | y) d\theta\)`
 
- it is variant to monotone transformations

`Practically:` `\(\bar{\theta}\)` is the typical souvenir you get from every bayesian software package.

## posterior median

`\(0.5 = \int_{\tilde{\theta}} p(\theta | y) d\theta\)`
 
- it is invariant to monotone transformations

`Practically:` `\(\tilde{\theta}\)` is another souvenir you get from every bayesian software package.

---

# Posterior summary measures

## posterior mode

`\(\hat{\theta} = arg \ max_\theta \ p(\theta | y)\)`
 
- it is variant to monotone transformations

`Practically:` `\(\hat{\theta}\)` is not a typical souvenir you get from every bayesian software package because it requires maximization.

---

# Posterior measure of variability

## Posterior variance

`\(\bar{\sigma}^2 = \int_{\theta} (\theta - \bar{\theta})^2 p(\theta | y) d\theta\)`

`Practically:` `\(\bar{\sigma}^2\)` is another souvenir you get from every bayesian software package.

---

# Posterior measures of uncertainty

`\([a, b] \backepsilon p(a \leq \theta \leq b | y) = 1 - \alpha\)`

## Equal tail credible interval

`\(p(\theta \leq a|y) = \frac{\alpha}{2}, \ p(\theta \geq b|y) = \frac{\alpha}{2}\)`

`Practically:` another souvenir you get from every bayesian software package.

- there might be `\(\theta\)` values with higher posterior probabilities outside `\([a, b]\)`

## Highest posterior density interval

`\([a, b] \backepsilon \forall \ \theta_1 \in [a, b], \theta_2 \notin [a, b], p(\theta_1 |y) \geq p(\theta_1 |y)\)`

`Practically:` not often reported.

- there are no `\(\theta\)` values with higher posterior probabilities outside `\([a, b]\)`

---

# Model comparison

The typical tool is `Bayes factor`. Other methods include;

- Deviance Information Criterion (DIC)

- Posterior Predictive Checks (PPC)

- Leave One Out cross validation (LOO)

- Weighted Akaike Information Criterion (WAIC)

---

class: center, middle

# An example

---

# The Data

## Monoculture Cyanobacteria Experiment

&lt;img src="pics/monoculture_experiment.PNG" width="70%" height="50%" style="display: block; margin: auto;" /&gt;

---

# The Data

## Biculture Cyanobacteria Experiment

&lt;img src="Pics/biculture_experiment.PNG" width="70%" height="50%" style="display: block; margin: auto;" /&gt;

---

# Interest

- effect of temperature on the growth of the cyanobacteria cultures over time.

&lt;iframe src = 'html/abd.html' scrolling='no' seamless='seamless' width = 900 height = 500&gt;


---

# A Verhulst growth model 

`\(\frac{dN}{dt} = N (r - AN)\)`, `\(A = \frac{r}{K}\)`

- `\(A\)`, `\(K\)` and `\(r\)` are the intra-specific effect, carrrying capcity and intrinsic growth rate respectively

- `\(N\)` and `\(t\)` are measured abundance of the cyanobacteria and time respectively

- `\(\frac{r N_0 e^{rt}}{r - AN_0 (1 + AN_0 e^{rt})}\)` is the solution to the above growth model

- `\(\frac{KN_0}{N_0 + (K - N_0) e^{-rt}}\)` is the solution expressed in terms of `\(r\)` and `\(K\)`

---

class: center, middle

# Frequentist analysis

---

## Notes

- we write a function for the solution to the model and fit this using `gnls` function in the `nlme` package.

- We will demonstrate the use of numerical solutions in the next example.

- Best to formulate the model in terms of `\(r\)` and `\(K\)`, since `\(A\)` is correlated with both

- We estimate `\(r\)`, `\(A\)` and `\(K\)` per temperature

---

## R Code using analytical solution


```r
*SSLVE &lt;- function(time, N0, r, K) {
*   numerator &lt;- K * N0
*   denominator &lt;- N0 + ((K - N0) * exp(-r * time))
*   mu &lt;- numerator / denominator
*   return(mu)
*} 


ver_model &lt;- gnls(ldensity ~ SSLVE(time = Time, N0, r, K), 
                data = b5_mono_data, 
                params = list(
                              N0 ~ 1, 
*                             r ~ Temperature,
*                             K ~ Temperature
                              ),
                start = c(4.2, 0.22, 0.23, 0.36, 9, 0, 0),
                correlation = NULL,
                na.action = na.omit
)
```

---

## Parameter Estimates

&lt;iframe src = 'html/est_freq.html' width = 900 height = 500&gt;

---

class: center, middle

# Bayesian analysis 

---

## Notes

* We will use the `rstan` package for the analysis

* [rstan](https://mc-stan.org/users/interfaces/rstan.html) = R interface to [Stan](https://mc-stan.org/)

* There is a special [group](https://stanecology.github.io/) and [guide](https://stanecology.github.io/) for Ecologist using Stan 

* Stan is written in blocks. The important ones are; `data{}`, `parameters{}`, `model{}`

* Other blocks are; `functions{}`, `transformed data{}`, `transformed parameters{}`, `generated quantities{}`

* Visit the [quick start guide](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), [user's guide](https://mc-stan.org/docs/2_22/stan-users-guide/index.html), [reference manual](https://mc-stan.org/docs/2_22/reference-manual/index.html) and [functions reference](https://mc-stan.org/docs/2_22/functions-reference/index.html) for more on the use of Stan

* First, we have to write the Stan code (good news for *C++* programmers) for the model before we compile and sample the model using `rstan` in R.

---

## The model in a familiar sense

* The likelihood/data = `\(\tilde{N_t} \sim N( \frac{r N_0 e^{rt}}{r - AN_0 (1 + AN_0 e^{rt})}, \sigma^2_N )\)`

* `\(r &gt; 0\)`, `\(K &gt; 0\)`, `\(\sigma^2_N &gt; 0\)` so we restrict them in the parameters block

* Priors = `\(r \sim N(0, 1)\)`, `\(K \sim N(0, 5)\)`, `\(\sigma^2_N \sim Cauchy(0, 5)\)`

* Priors were chosen based on EDA and known properties of these parameters

---

## Stan Code (data block)


```rstan
// input data

*data {

    int&lt;lower=0&gt; N_obs; // number of observations
    
    //number of treatment groups
    int&lt;lower=0&gt; n_trt;
    
    // response
    real y_obs[N_obs];
    
    //design matrix for the whole treatment
    matrix[N_obs, n_trt] trt;
    
    //time vector
    vector[N_obs] time;
}
```
---

## Stan Code (parameters block)


```rstan
// The parameters accepted by the model

*parameters {
  
  vector&lt;lower=0&gt;[n_trt] K; //Ks
  vector&lt;lower=0&gt;[n_trt] r; //rs
  real&lt;lower=0&gt; N0; //N0
  real&lt;lower=0&gt; sigma_N; //residual variance  

}
```

---

## Stan Code (transformed parameters block)


```rstan
// The model to be estimated.

*transformed parameters {

  vector[N_obs] yhat_dens; //density mean value
  vector[N_obs] numerator;
  vector[N_obs] denominator;
  vector[N_obs] K_trt;
  vector[N_obs] r_trt;
  
  // estimate K and r per treatment level
  K_trt = trt * K;
  r_trt = trt * r;
  numerator = N0 * K_trt;
  
  //mean function using the analytical solution
  for (i in 1:N_obs) {
    //mean function for density
    denominator[i] = N0 + ((K_trt[i] - N0) * exp((-1 * r_trt[i]) * time[i]));
    yhat_dens[i] = numerator[i] / denominator[i];
    
  }
    
}
```

---

## Stan Code (model and generated quantities block)


```rstan
// The model to be estimated.

*model {
  
  sigma_N ~ cauchy(0, 5); //prior for variance of abundance
  r ~ std_normal(); //prior for rs
  K ~ normal(0, 5); //prior for Ks
  
  for(i in 1:N_obs)
    y_obs[i] ~ normal(yhat_dens[i], sigma_N); //the likelihood
  
}

*generated quantities {
  
   vector&lt;lower=0&gt;[n_trt] A;
  
  for(i in 1:n_trt)
    A[i] = r[i] / K[i]; // estimating the 
  
}
```

* this stan file is saved as **verhulst.stan** in the **stan_models** directory
* Next we formulate and sample the posterior distribution using functions in the `rstan` package

---

## R Code


```r
X_obs &lt;- b5_mono_data_nona %&gt;% 
  model.matrix(~I(Time^0.5):T_18 + I(Time^3):T_2022 + 
                 I((Time^3) * log(Time)):T_2022 +
                 T_18 + T_20 + T_22, data = . )
*verhulst_data &lt;- list(
* N_obs = nrow(X_obs),
* y_obs = b5_mono_data_nona[, "ldensity"],
* trt = b5_mono_data_nona[, c("T_18", "T_20", "T_22")],
* time = b5_mono_data_nona$Time,
* n_trt = ncol(trt_mat)
)
init &lt;- list( list(r = c(0.22, 0.23, 0.35)/2, K = c(9.72, 11.50, 10.99)/2), 
              list(r = c(0.22, 0.23, 0.35)/3, K = c(9.72, 11.50, 10.99)/3
           )
# formulate model from stan file
ret_mod &lt;- stanc("stan_models/verhulst.stan") 
sm &lt;- stan_model(stanc_ret = ret_mod, verbose = FALSE)
#sample from the formulate stan object
*sm_fit &lt;- sampling(sm,
*                  data = verhulst_data,
*                  iter = 3000,
*                  thin = 1,
*                  control = list(max_treedepth = 13, adapt_delta = 0.98),
*                  init = init,
*                  chains = 2
*                 )
```

---

## Important

* Always check for convergence! Stan will also warn you if there were problems during sampling.


```r
stan_trace(stan_verhulst, pars = c("K", "r", "A"))
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="index_files/figure-html/unnamed-chunk-7-1.png" alt="Trace plots for the parameters" width="55%" height="45%" /&gt;
&lt;p class="caption"&gt;Trace plots for the parameters&lt;/p&gt;
&lt;/div&gt;

---

## Parameter Estimates

&lt;iframe src = 'html/est_bay.html' scrolling='no' seamless='seamless' width = 900 height = 500&gt;

---

## Comparison of results

&lt;iframe src = 'html/est_bayfreq.html' scrolling='no' seamless='seamless' width = 900 height = 500&gt;

---

# A Lotka-Voterra competition model

`\(\frac{\delta \mathbf{N}}{\delta t} = \mathbf{N} \mathbf{r} (1 - \mathbf{A} \mathbf{N})\)`

* `\(\mathbf{N}\)` is a vector of abundance for the cyanobacteria species, `\(\mathbf{r}\)` is a vector of growth rates, `\(\mathbf{A}\)` is a matrix of inter- and intraspecific effects. 

* for 2 species we have, `\(\frac{d \mathbf{N}}{dt}  =  \left(\begin{array}{c} N_1 \\ N_2 \end{array} \right) \left(\begin{array}{c} r_1 \\ r_2 \end{array} \right) \left[1 - \left( \begin{array}{cc} \alpha_{11} &amp; \alpha_{12}\\ \alpha_{21} &amp; \alpha_{22} \end{array} \right)\left(\begin{array}{c} N_1 \\ N_2 \end{array} \right) \right]\)`

* `\(\alpha_{11}\)`, `\(\alpha_{22}\)` are intraspecific effects (effect of a species on itself), while `\(\alpha_{12}\)`, `\(\alpha_{21}\)` are interspecific effects (effect of a species on the other species).

* note that `\(\frac{\alpha_{12}}{\alpha_{22}} &lt; 1 &lt; \frac{\alpha_{11}}{\alpha_{21}}\)` for a feasible coexistence equilibrium.


---

class: center, middle

# Frequentist analysis 

## Let's skip this for now

---

class: center, middle

# Bayesian analysis 

---

## The model in a familiar sense

* The likelihood/data = `\(\left(\begin{array}{c} N_1 \\ N_2 \end{array} \right) \sim BVN\left( \left( \begin{array}{c} \int{\frac{dN_1}{dt}} \\ \int{\frac{dN_2}{dt}} \end{array}\right)  , \Sigma_N \right)\)`, `\(\Sigma_N = \left( \begin{array}{cc} \sigma^2_{N_1} &amp; \rho\sigma_{N_1}\sigma_{N_2}\\ \rho\sigma_{N_1}\sigma_{N_2} &amp; \sigma^2_{N_2} \end{array} \right)\)`

* `\(\frac{dN_1}{dt} = N_1r_1 (1 - (\alpha_{11}N_1 +  \alpha_{12}N_2))\)` and `\(\frac{dN_2}{dt} = N_2r_2 (1 - (\alpha_{21}N_1 +  \alpha_{22}N_2))\)`

* `\(r_i &gt; 0\)`, `\(\alpha_{11} &gt; \alpha_{21}\)`, `\(\alpha_{22} &gt; \alpha_{12}\)`, `\(\alpha_{21} &gt; 0\)`, `\(\alpha_{12} &gt; 0\)` , `\(\sigma^2_{N_i} &gt; 0\)`, `\(i = 1, 2\)` so we restrict them in the parameters block.

* Priors = `\(r_i \sim N(0, 1)\)`, `\(\alpha_{12} \sim N(0, 1)\)`, `\(\alpha_{21} \sim N(0, 1)\)`, `\(\alpha_{11}, \alpha_{22}\)` get Stan default priors

* We parameterise `\(\Sigma_N\)` in Cholesky form for stability (see [this](https://mc-stan.org/docs/2_22/stan-users-guide/multivariate-outcomes.html) for more details on this approach) thus, `\(\sigma^2_{N_i} \sim Cauchy(0, 5)\)` and the correlation matrix gets a LKJ prior (see [this](https://mc-stan.org/docs/2_22/functions-reference/lkj-correlation.html) page for more details on the LKJ correlation distribution).

---

## Stan Code (functions block)


```rstan
// LVE
functions { 
  
  real[] biLVE(real t,  //time 
             real[] N,//state or abundance
*            real[] tetha, //parameters
             real[] x_r, //data (real)
             int[] x_i //data (integer)
             ) {
               
      real dNdt[2];
      
      
      dNdt[1] =  N[1] * tetha[1] * (1 - (tetha[3]*N[1] + tetha[5]*N[2]) );
      dNdt[2] =  N[2] * tetha[2] * (1 - (tetha[4]*N[1] + tetha[6]*N[2]) );
      return dNdt;
  }
  
}
```

---

## Stan Code (data block)


```rstan
data {
  
  int&lt;lower=0&gt; N_obs; //N_obs
  int&lt;lower=0&gt; T; //number of unique timepoints
  
  vector[2] N[N_obs]; //observed data
  int&lt;lower=0&gt; nsp; //number of species
  
  real t0; // first time point
  real ts[T]; // time to integrate ODE over
  vector[N_obs] time_obs; //observed time point
 
}
```

---

## Stan Code (transformed data and parameters block)


```rstan
transformed data {
  real x_r[0]; //for the ODE integrator function
  int x_i[0]; //for the ODE integrator function
}

parameters {
  
  //growth rate parameters for the LVE model
  real&lt;lower=0&gt; r[2];
  
  //interspecific parameters
  real&lt;lower=0&gt; alpha12; 
  real&lt;lower=0&gt; alpha21; 
  
  //intraspecific parameters
  real&lt;lower = alpha21&gt; alpha11; 
  real&lt;lower = alpha12&gt; alpha22; 
  
  real&lt;lower=0&gt; N0[nsp]; //starting values for both 
  vector&lt;lower=0&gt;[nsp] sigma; //variance for the two species
  cholesky_factor_corr[nsp] L_Omega; //for the correlation matrix
  
}
```

---

## Stan Code (transformed parameters)


```rstan

transformed parameters {
  
    real N_LVE[T, nsp];
    matrix[T, nsp] N_exp;
    vector[nsp] N_hat[N_obs];
    real tetha[6];
  
*   tetha[1] = r[1];
*   tetha[2] = r[2];
*   tetha[3] = alpha11;
*   tetha[4] = alpha21;
*   tetha[5] = alpha12;
*   tetha[6] = alpha22;
  
*   N_LVE = integrate_ode_rk45(biLVE, N0, t0, ts, tetha, x_r, x_i);
    N_exp = to_matrix(N_LVE);
    
    //setting the expected N_exp to N_LVE at the required time points
    for(k in 1:T) {
      for(j in 1:N_obs) {
        
        if(ts[k] == time_obs[j]) N_hat[j] = to_vector(N_exp[k, ]);
        
      }
    }
}
```

---

## Stan Code (model block)


```rstan
model {
  
  matrix[2, 2] L_Sigma;
  
  //priors for the LVE parameters
  
  //interspecific effects
  alpha12 ~ std_normal(); 
  alpha21 ~ std_normal();
  
  r ~ std_normal(); //growth rates
  
  //priors for starting value
  N0 ~ normal(0, 5);
  
  //priors for the variance-covariance matrix
  //of the response in cholesky decomposition form
  L_Omega ~ lkj_corr_cholesky(4);
  L_Sigma = diag_pre_multiply(sigma, L_Omega);
  sigma ~ cauchy(0, 2.5);
  
  
  //and build the log-likelihood
  N ~ multi_normal_cholesky(N_hat, L_Sigma);
  
}
```

---

Stan Code (generated quantities)


```rstan

generated quantities {
  
  matrix[2, 2] Sigma;
  matrix[2, 2] L_Sigma;
  real cor;
  
  //reconstructing the variance-covariance matrix from the chole
  L_Sigma = diag_pre_multiply(sigma, L_Omega);
  Sigma = L_Sigma * L_Sigma';
  cor = Sigma[1,2] / pow(Sigma[1,1] * Sigma[2,2], 0.5);
  
}
```

---

## Comparison of results

&lt;iframe src = 'html/est_bayLVE.html' scrolling='no' seamless='seamless' width = 900 height = 500&gt;


---

## A simple model comparison exercise
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  /* Replace <script> tags in slides area to make them executable
   *
   * Runs after post-processing of markdown source into slides and replaces only
   * <script>s on the last slide of continued slides using the .has-continuation
   * class added by xaringan. Finally, any <script>s in the slides area that
   * aren't executed are commented out.
   */
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container:not(.has-continuation) script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
  var scriptsNotExecuted = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container.has-continuation script'
  );
  if (!scriptsNotExecuted.length) return;
  for (var i = 0; i < scriptsNotExecuted.length; i++) {
    var comment = document.createComment(scriptsNotExecuted[i].outerHTML)
    scriptsNotExecuted[i].parentElement.replaceChild(comment, scriptsNotExecuted[i])
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
